# Flash Attention Educational
Implementation of flash attention for educational purposes. This does not implement dropout and masking to keep it simple.

This is under construction.

This will include:

- [x] implemenation in numba
    - [x] forward
    - [x] backward
- [ ] implementation in C++
    - [ ] forward -- still not the same result as standard attention
- [ ] implementation in tritongit remote set-url origin
- [ ] Profile + optimize
